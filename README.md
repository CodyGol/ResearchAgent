# ResearchAgentv2

A production-grade recursive deep-research agent system with a Generative UI frontend. Plans research paths, executes searches, critiques findings, and synthesizes comprehensive reports.

## ðŸš€ Quick Start

### Option 1: Web UI (Recommended)

1. **Start the frontend**:
   ```bash
   cd research-client
   npm install
   npm run dev
   ```

2. **Configure backend URL** in `research-client/.env.local`:
   ```env
   NEXT_PUBLIC_BACKEND_URL=https://research-agent-v2-69957378560.us-central1.run.app
   ```

3. **Open browser**: http://localhost:3000

### Option 2: Command Line

```bash
python run_research.py "Your research query here"
```

### Option 3: REST API (Streaming)

The API uses **NDJSON streaming** for real-time progress:

```bash
curl -X POST https://research-agent-v2-69957378560.us-central1.run.app/research \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the latest developments in quantum computing?"}' \
  --no-buffer
```

**Response**: Streams NDJSON events (`log`, `result`, `done`) for real-time updates.

## Architecture

ResearchAgentv2 uses a LangGraph state machine with four core nodes:

1. **Planner**: Analyzes user query and generates structured research plan with domain filters
2. **Researcher**: Executes web searches via Tavily API with retry logic and spam filtering
3. **Critic**: Evaluates research quality and decides if refinement is needed
4. **Writer**: Synthesizes final report from approved research

The Critic node implements a recursive loop: if quality is insufficient, it routes back to Researcher for refinement (up to `MAX_RESEARCH_ITERATIONS`).

**System Components:**
- **Backend**: FastAPI service (`api.py`) with queue-based streaming, deployed on Google Cloud Run
- **Frontend**: Next.js Generative UI (`research-client/`) with Edge runtime and NDJSON streaming
- **Agent**: LangGraph state machine with recursive refinement
- **Streaming**: NDJSON event stream with shielded event generator (prevents GeneratorExit)
- **Observability**: LangSmith tracing (always finalizes) and structured logging
- **Evaluation**: Automated testing with LLM-as-a-Judge (`run_eval.py`)

See [docs/architecture.md](docs/architecture.md) for the full architecture diagram.

## Setup

### Prerequisites

- Python 3.12+
- `uv` package manager (or `pip`)
- Node.js 18+ (for frontend)

### Installation

1. **Clone the repository**

2. **Create `.env` file** with your API keys:
   ```env
   ANTHROPIC_API_KEY=sk-ant-...
   TAVILY_API_KEY=tvly-...
   SUPABASE_URL=https://xxx.supabase.co (optional)
   SUPABASE_KEY=xxx (optional)
   LANGCHAIN_TRACING_V2=true (optional)
   LANGCHAIN_API_KEY=ls-... (optional)
   LANGCHAIN_PROJECT=ResearchAgentv2 (optional)
   ```

3. **Install Python dependencies**:
   ```bash
   uv sync
   # or
   pip install -r requirements.txt
   ```

4. **Install frontend dependencies** (if using web UI):
   ```bash
   cd research-client
   npm install
   ```

See [SETUP.md](SETUP.md) for detailed setup instructions.

## Usage

### Web UI

The **Deep Research Console** provides a terminal-style interface:
- Non-blocking long-running requests (up to 10 minutes)
- Real-time execution timer using `requestAnimationFrame`
- Smooth fade-in animations for results
- React Markdown rendering with error boundaries

### Command Line

```bash
# Run with default query
python run_research.py

# Run with custom query
python run_research.py "Your research question here"
```

### Python API

```python
from graph import create_graph
import asyncio

async def research(query: str):
    graph = create_graph()
    app = graph.compile()
    
    initial_state = {
        "user_query": query,
        "research_plan": None,
        "research_results": None,
        "critique": None,
        "final_report": None,
        "current_node": "planner",
        "iteration_count": 0,
        "error": None,
    }
    
    result = await app.ainvoke(initial_state)
    return result["final_report"]

# Run it
report = asyncio.run(research("Your research query here"))
print(report.content)
```

### Evaluation System

Run automated evaluation against golden dataset:

```bash
python run_eval.py
```

This evaluates the agent against test cases in `tests/golden_dataset.json` using LLM-as-a-Judge, providing accuracy metrics and performance analysis.

See [USAGE_GUIDE.md](USAGE_GUIDE.md) for comprehensive usage examples, integration patterns, and best practices.

## Project Structure

```
.
â”œâ”€â”€ api.py                   # Production FastAPI service (Cloud Run)
â”œâ”€â”€ config.py                # Pydantic-settings configuration
â”œâ”€â”€ state.py                 # AgentState and Pydantic models
â”œâ”€â”€ graph.py                 # LangGraph StateGraph definition
â”œâ”€â”€ run_research.py          # CLI script for research
â”œâ”€â”€ run_eval.py              # Evaluation system with LLM-as-a-Judge
â”œâ”€â”€ server.py                # Legacy FastAPI server (use api.py for production)
â”œâ”€â”€ Dockerfile               # Production Docker image for Cloud Run
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ pyproject.toml           # Project configuration (uv)
â”‚
â”œâ”€â”€ nodes/                   # Agent node implementations
â”‚   â”œâ”€â”€ planner.py           # Research plan generation
â”‚   â”œâ”€â”€ researcher.py        # Web search execution
â”‚   â”œâ”€â”€ critic.py            # Quality evaluation
â”‚   â””â”€â”€ writer.py            # Report synthesis
â”‚
â”œâ”€â”€ tools/                   # Utility tools
â”‚   â””â”€â”€ search.py            # Tavily search with retry & spam filtering
â”‚
â”œâ”€â”€ db/                      # Supabase integration
â”‚   â”œâ”€â”€ client.py            # Database client
â”‚   â”œâ”€â”€ models.py            # Database models
â”‚   â”œâ”€â”€ repository.py        # Data access layer
â”‚   â””â”€â”€ schema.sql           # Database schema
â”‚
â”œâ”€â”€ utils/                   # Utilities
â”‚   â”œâ”€â”€ observability.py     # Tracing and logging
â”‚   â”œâ”€â”€ pii_redaction.py     # PII redaction
â”‚   â””â”€â”€ serialization.py     # JSON serialization helpers
â”‚
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ test_logic.py        # Unit tests (blacklist, serialization, state)
â”‚   â””â”€â”€ golden_dataset.json  # Evaluation test cases
â”‚
â”œâ”€â”€ research-client/         # Next.js frontend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx         # Main UI component
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ research/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts # API proxy with timeout
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â””â”€â”€ ErrorBoundary.tsx
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.md     # Architecture diagram
â”‚
â”œâ”€â”€ test_supabase.py         # Supabase connection test
â”œâ”€â”€ fix_env.py               # .env file diagnostic tool
â””â”€â”€ pytest.ini               # Pytest configuration
```

## Features

### Core Agent
- âœ… **Recursive Refinement**: Self-correcting research with quality checks
- âœ… **Domain Filtering**: Prioritizes primary sources (arxiv.org, github.com, etc.)
- âœ… **Spam Filtering**: Blacklists SEO blogs (Medium, LinkedIn, etc.)
- âœ… **Structured Output**: Pydantic V2 models for all data
- âœ… **Error Handling**: Retryable vs Fatal error categorization

### Production Features
- âœ… **FastAPI Service**: Production-ready API with health checks and CORS
- âœ… **Streaming Architecture**: NDJSON event streaming with queue-based shielding
- âœ… **Docker Support**: Optimized container for Cloud Run (Python 3.12-slim)
- âœ… **Next.js Frontend**: Generative UI with Edge runtime and streaming support
- âœ… **LangSmith Tracing**: Full observability with guaranteed trace finalization
- âœ… **Supabase Integration**: Caching and persistence
- âœ… **Evaluation System**: Automated testing with LLM-as-a-Judge

### Observability
- âœ… **Structured Logging**: JSON logs with PII redaction
- âœ… **LangSmith Integration**: Real-time trace viewing
- âœ… **Performance Metrics**: Latency tracking per node
- âœ… **Error Tracking**: Categorized errors with tracebacks

## Development Status

**Current Phase**: Production Ready âœ…

- âœ… State machine scaffolding
- âœ… Real LLM integration (Claude 4.5 Sonnet)
- âœ… Real Tavily API integration
- âœ… Supabase integration (caching & persistence)
- âœ… Retry logic with structured error handling
- âœ… Observability/tracing with PII redaction
- âœ… LangSmith integration
- âœ… Production FastAPI service
- âœ… Next.js Generative UI
- âœ… Evaluation system
- âœ… Docker/Cloud Run deployment
- âœ… Unit tests (logic verification)
- âœ… CI/CD pipeline

## Deployment

### Backend (Google Cloud Run)

```bash
# Build and push
docker build -t gcr.io/YOUR_PROJECT/research-agent:latest .
docker push gcr.io/YOUR_PROJECT/research-agent:latest

# Deploy
gcloud run deploy research-agent \
  --image gcr.io/YOUR_PROJECT/research-agent:latest \
  --platform managed \
  --region us-central1 \
  --set-env-vars TAVILY_API_KEY=xxx,ANTHROPIC_API_KEY=xxx \
  --timeout 600 \
  --memory 2Gi \
  --cpu 2 \
  --allow-unauthenticated
```

**Health Check**: `/health` endpoint returns `{"status": "ok"}` for Cloud Run probes.

### Frontend (Vercel)

```bash
cd research-client
vercel deploy
```

Set environment variable: `NEXT_PUBLIC_BACKEND_URL`

## Design Principles

This project follows strict architectural principles:

- **Composition > Inheritance**: Node-based architecture
- **Schema > Guesswork**: Pydantic V2 models for all data
- **Tracing > Logging**: Structured observability with PII redaction
- **Idempotency**: All operations are retryable
- **Fail Loudly**: Structured error handling (Retryable vs Fatal)
- **Observability First**: LangSmith tracing for all LLM calls
- **Production Ready**: Docker, health checks, error boundaries

## Documentation

- **[SETUP.md](SETUP.md)**: Detailed setup instructions
- **[USAGE_GUIDE.md](USAGE_GUIDE.md)**: Comprehensive usage examples
- **[docs/architecture.md](docs/architecture.md)**: System architecture diagrams
- **[research-client/SETUP.md](research-client/SETUP.md)**: Frontend setup

## Testing

Run the test suite:

```bash
# Logic tests (no API calls)
uv run pytest tests/test_logic.py -v

# Full evaluation (requires API keys)
python run_eval.py
```

